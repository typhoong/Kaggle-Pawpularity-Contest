{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4ccfc2e",
   "metadata": {
    "id": "HB6tcaFW_H3T",
    "papermill": {
     "duration": 0.022571,
     "end_time": "2021-11-28T13:25:02.216142",
     "exception": false,
     "start_time": "2021-11-28T13:25:02.193571",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "fork https://www.kaggle.com/nishantrajadhyaksha/pawpularity-pytorchlightning-w-b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7dc9d2d7",
   "metadata": {
    "id": "qwvvNvo_2LH5",
    "papermill": {
     "duration": 9.927229,
     "end_time": "2021-11-28T13:25:12.217576",
     "exception": false,
     "start_time": "2021-11-28T13:25:02.290347",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "from pprint import pprint\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as T\n",
    "from box import Box\n",
    "from timm import create_model\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import optuna, copy\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.utilities.seed import seed_everything\n",
    "from pytorch_lightning import callbacks\n",
    "from pytorch_lightning.callbacks.progress import ProgressBarBase\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning import LightningDataModule, LightningModule\n",
    "# from pytorch_lightning.loggers import WandbLogger\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "# import wandb\n",
    "import cv2\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.svm import SVR\n",
    "import shutil\n",
    "import pickle\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "from torch.utils.data import TensorDataset\n",
    "import time\n",
    "import gc\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06331350-89b7-44c5-966b-281c7bdd4dcb",
   "metadata": {},
   "source": [
    "# Meta info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4813213",
   "metadata": {
    "id": "spieGAU32LH6",
    "papermill": {
     "duration": 0.02556,
     "end_time": "2021-11-28T13:25:12.306938",
     "exception": false,
     "start_time": "2021-11-28T13:25:12.281378",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "drive_root = '..'\n",
    "TRAIN_DIR = f\"{drive_root}/input/petfinder-pawpularity-score/train\"\n",
    "TEST_DIR = f\"{drive_root}/input/petfinder-pawpularity-score/test\"\n",
    "DENSE_FEATURES = [\n",
    "    'Subject Focus',\n",
    "    'Eyes',\n",
    "    'Face',\n",
    "    'Near',\n",
    "    'Action',\n",
    "    'Accessory',\n",
    "    'Group',\n",
    "    'Collage',\n",
    "    'Human',\n",
    "    'Occlusion',\n",
    "    'Info',\n",
    "    'Blur',\n",
    "    'img_long_axis'\n",
    "]\n",
    "df_train = pd.read_csv(f\"{drive_root}/input/petfinder-pawpularity-score/train.csv\")\n",
    "df_test = pd.read_csv(f\"{drive_root}/input/petfinder-pawpularity-score/test.csv\")\n",
    "df_train['filepath'] = df_train.Id.apply(lambda x :f\"{TRAIN_DIR}/{x}.jpg\" )\n",
    "df_test['filepath'] = df_test.Id.apply(lambda x :f\"{TEST_DIR}/{x}.jpg\" )\n",
    "\n",
    "long_axis_df_path = f\"{drive_root}/input/pet-train-long-axis/df_train_w_long_axis.csv\"\n",
    "long_axis_max = 1280\n",
    "if os.path.isfile(long_axis_df_path):\n",
    "    df_train = pd.read_csv(long_axis_df_path)\n",
    "else:\n",
    "    df_train['img_long_axis'] = df_train.Id.apply(lambda x : max(cv2.imread(f\"{TRAIN_DIR}/{x}.jpg\" ).shape[:2]))\n",
    "    df_train['img_long_axis'] /=long_axis_max\n",
    "    df_train['img_long_axis'] = df_train['img_long_axis'].astype(np.float32)\n",
    "    df_train['filepath'] = df_train.Id.apply(lambda x :f\"{TRAIN_DIR}/{x}.jpg\" )\n",
    "    \n",
    "df_test['img_long_axis'] = df_test.Id.apply(lambda x : max(cv2.imread(f\"{TEST_DIR}/{x}.jpg\" ).shape[:2]))\n",
    "df_test['img_long_axis'] /=long_axis_max\n",
    "df_test['img_long_axis'] = df_test['img_long_axis'].astype(np.float32)\n",
    "df_test['filepath'] = df_test.Id.apply(lambda x :f\"{TEST_DIR}/{x}.jpg\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356a8b01-b1b2-401a-b4bb-ccdece80b6f6",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "216f32ca",
   "metadata": {
    "id": "8cg0XTpW94Hi",
    "papermill": {
     "duration": 0.024159,
     "end_time": "2021-11-28T13:25:12.393156",
     "exception": false,
     "start_time": "2021-11-28T13:25:12.368997",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.eps = eps\n",
    "        \n",
    "    def forward(self,yhat,y):\n",
    "        loss = torch.sqrt(self.mse(yhat,y) + self.eps)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b0e166-1040-44ba-b8ef-eeaea0ebfddb",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11894965",
   "metadata": {
    "id": "5TZT70L4pi4u",
    "papermill": {
     "duration": 0.027435,
     "end_time": "2021-11-28T13:25:12.623895",
     "exception": false,
     "start_time": "2021-11-28T13:25:12.596460",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, x, y=None):#, image_size=224):\n",
    "        self._X = x\n",
    "        self._y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        feature = self._X[idx]\n",
    "        if self._y is not None:\n",
    "            label = self._y[idx]\n",
    "            return feature, label\n",
    "        return feature\n",
    "\n",
    "class PetfinderDataModule(LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_features, train_y, val_features, val_y, cfg,):\n",
    "        super().__init__()\n",
    "        self._train_features = train_features\n",
    "        self._val_features = val_features\n",
    "        self._train_y = train_y\n",
    "        self._val_y = val_y\n",
    "        self._cfg = cfg\n",
    "\n",
    "    def __create_dataset(self, train=True):\n",
    "        if train==True:\n",
    "            return CustomDataset(self._train_features, self._train_y)\n",
    "        else:\n",
    "            return CustomDataset(self._val_features, self._val_y)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        dataset = self.__create_dataset(True)\n",
    "        return DataLoader(dataset, **self._cfg.train_loader)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        dataset = self.__create_dataset(False)\n",
    "        return DataLoader(dataset, **self._cfg.val_loader)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5939613b-ab00-4961-9ab5-343c39592d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper():\n",
    "    def __init__(self, patience: int, mode:str)-> None:\n",
    "        self.patience = patience\n",
    "        self.mode = mode\n",
    "\n",
    "        # Initiate\n",
    "        self.patience_counter = 0\n",
    "        self.stop = False\n",
    "        self.best_loss = np.inf\n",
    "        \n",
    "    def check_early_stopping(self, loss: float)-> None:\n",
    "        loss = -loss if self.mode == 'max' else loss  # get max value if mode set to max\n",
    "\n",
    "        if loss >= self.best_loss:\n",
    "            # got better score\n",
    "            self.patience_counter += 1\n",
    "            \n",
    "            if self.patience_counter == self.patience:\n",
    "                self.stop = True  # end\n",
    "\n",
    "        elif loss < self.best_loss:\n",
    "            # got worse score\n",
    "            self.patience_counter = 0\n",
    "            self.best_loss = loss\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d56e0934",
   "metadata": {
    "id": "5FC8MKiq2LH7",
    "papermill": {
     "duration": 0.031668,
     "end_time": "2021-11-28T13:25:12.674744",
     "exception": false,
     "start_time": "2021-11-28T13:25:12.643076",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = {'seed': 2021,\n",
    "          'root': f\"{drive_root}\", \n",
    "          'n_splits': 10,\n",
    "          'epoch': 1000,#30,\n",
    "          'patience':10,\n",
    "          'trainer': {\n",
    "              'gpus': 1,\n",
    "              'auto_lr_find':False,\n",
    "              'accumulate_grad_batches': 1,\n",
    "              'progress_bar_refresh_rate': 1,\n",
    "              'fast_dev_run': False,\n",
    "              'num_sanity_val_steps': 0,\n",
    "              'resume_from_checkpoint': None,\n",
    "#               'accelerator':'ddp',\n",
    "#               'strategy':'ddp'\n",
    "          },\n",
    "          'transform':{\n",
    "              'name': 'albu',\n",
    "              'image_size': 384,#224,#384,\n",
    "              'squared':True,\n",
    "              'aug':'rotate,shift,scale,perspective,more',\n",
    "          },\n",
    "          'train_loader':{\n",
    "              'batch_size': 512,#16,#64,\n",
    "              'shuffle': True,\n",
    "              'num_workers': 1,\n",
    "              'pin_memory': False,\n",
    "              'drop_last': True,\n",
    "            \n",
    "          },\n",
    "          'val_loader': {\n",
    "              'batch_size': 512,#16,#64,\n",
    "              'shuffle': False,\n",
    "              'num_workers': 8,\n",
    "              'pin_memory': False,\n",
    "              'drop_last': False\n",
    "         },\n",
    "          'save_discript':'Sigmoid_Bayesian',\n",
    "          'model':{\n",
    "              'name': 'swin_large_patch4_window12_384_in22k',#'deit_base_distilled_patch16_384',#'swin_tiny_patch4_window7_224',#\n",
    "              'img_feature_dim':128,\n",
    "              'output_dim': 1,\n",
    "              'first_drop': 0.5,\n",
    "              'second_drop': 0.5,\n",
    "              'third_drop': 0.5,\n",
    "              'last_drop':0.5,\n",
    "              'activation':'nn.SELU',\n",
    "              'quantile_num':512,\n",
    "          },\n",
    "          'optimizer':{\n",
    "              'name': 'optim.AdamW',\n",
    "              'AdamW':{\n",
    "                  'lr': 1e-5,\n",
    "                  'betas': (1,1),\n",
    "                  'weight_decay': 0.01,\n",
    "                  'amsgrad': False,\n",
    "\n",
    "                },\n",
    "          },\n",
    "          'scheduler':{\n",
    "              'name': 'optim.lr_scheduler.CosineAnnealingWarmRestarts',\n",
    "              'CosineAnnealingWarmRestarts':{\n",
    "                  'T_0': 4,\n",
    "                  'T_mult':2,\n",
    "                  'eta_min': 1e-7,\n",
    "              }\n",
    "          },\n",
    "          'loss': 'QuantileLoss',#'nn.BCEWithLogitsLoss',\n",
    "          'metric': 'RMSELoss',\n",
    "\n",
    "          'svr':{\n",
    "              'C':0.5,\n",
    "          },\n",
    "          'quantile':{\n",
    "              'quantile_num':500,\n",
    "              'batch_size':1024,\n",
    "              'epoch':1000,\n",
    "          },\n",
    "          'svr_train':False,\n",
    "          'swin_train':False\n",
    "        \n",
    "}\n",
    "\n",
    "config = Box(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf693d3d-8a68-407f-a309-4fcb5fd100b2",
   "metadata": {},
   "source": [
    "# Setting arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ae2876e",
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1637508005015,
     "user": {
      "displayName": "Daewoo Myoung",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhyUBzKneZcIle1t7HXIAaL548lK9ndvDFCUhROwQ=s64",
      "userId": "15525163033989858669"
     },
     "user_tz": -540
    },
    "id": "b-nCf5Ze-6im",
    "outputId": "554d312b-fec4-402a-bb40-a442a7824d56",
    "papermill": {
     "duration": 0.029591,
     "end_time": "2021-11-28T13:25:12.872125",
     "exception": false,
     "start_time": "2021-11-28T13:25:12.842534",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 2021\n"
     ]
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)\n",
    "seed_everything(config.seed)\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "model_code = 'swin_2021'\n",
    "import random\n",
    "torch.manual_seed(config.seed)\n",
    "torch.cuda.manual_seed(config.seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(config.seed)\n",
    "random.seed(config.seed)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "skf = StratifiedKFold(\n",
    "    n_splits=config.n_splits, shuffle=True, random_state=config.seed\n",
    ")\n",
    "activation_list = ['nn.LogSigmoid', 'nn.Hardswish',  'nn.PReLU', 'nn.ReLU', 'nn.ELU', 'nn.ReLU6', 'nn.SELU', 'nn.CELU']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "db19f965-ab9a-4e96-9bbb-a0f863744ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    swin_scores=[]\n",
    "    val_scores=[]\n",
    "    config.optimizer.AdamW.lr = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n",
    "    config.model.h1 = trial.suggest_categorical('h1', [32, 64, 128, 256, 512, 1024, 2048])\n",
    "    config.model.h2 = trial.suggest_categorical('h2', [32, 64, 128, 256, 512, 1024, 2048])  \n",
    "    config.patience =  trial.suggest_categorical('patience', [5, 10, 20, 30, 50])\n",
    "    config.model.activation = trial.suggest_categorical('activation', activation_list)\n",
    "    config.model.first_drop = trial.suggest_float('first_drop', 0, 0.9)\n",
    "    config.model.second_drop = trial.suggest_float('second_drop', 0, 0.9)\n",
    "    config.model.third_drop = trial.suggest_float('third_drop', 0, 0.9)\n",
    "    \n",
    "    beta1 = trial.suggest_float('beta1', 0, 1)\n",
    "    beta2 = trial.suggest_float('beta2', 0, 1)\n",
    "    threshold = 0.5\n",
    "    binary_th = trial.suggest_int('binary_th', 80, 100)\n",
    "    config.optimizer.AdamW.betas = (beta1, beta2)\n",
    "    config.optimizer.AdamW.weight_decay = trial.suggest_float('weight_decay', 0, 1)\n",
    "    config.optimizer.AdamW.amsgrad = trial.suggest_categorical('amsgrad', [True, False])\n",
    "    config.scheduler.CosineAnnealingWarmRestarts.T_0 = trial.suggest_int('T_0', 1, 100)\n",
    "    config.scheduler.CosineAnnealingWarmRestarts.T_mult = trial.suggest_int('T_mult', 1, 100)\n",
    "    config.scheduler.CosineAnnealingWarmRestarts.eta_min = trial.suggest_loguniform('eta_min', 1e-10, 1e-5)\n",
    "    df_train['binary'] = df_train.Pawpularity.apply(lambda x : int(x >= binary_th))\n",
    "    model_save_dir = f'{drive_root}/output/weights/final_weights/{model_code}/'\n",
    "\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(df_train[\"Id\"], df_train[\"Pawpularity\"])):\n",
    "        early_stopper = EarlyStopper(patience = config.patience, mode='min')\n",
    "        val_min_score = 1e+100\n",
    "        train_df = df_train.loc[train_idx].reset_index(drop=True)\n",
    "        val_df = df_train.loc[val_idx].reset_index(drop=True)\n",
    "\n",
    "        train_ratio = 100 * sum(df_train['binary'])/len(df_train['binary'])\n",
    "        val_ratio = 100 * sum(val_df['binary'])/len(val_df['binary'])\n",
    "\n",
    "        # Load embed features\n",
    "        val_predicts = np.load(f'{model_save_dir}val_predicts_fold{fold}.npy')\n",
    "        train_embed_features = np.load(f'{model_save_dir}train_embed_org_fold{fold}.npy')\n",
    "        val_embed_features = np.load(f'{model_save_dir}val_embed_org_fold{fold}.npy')\n",
    "        train_embed_flip_features = np.load(f'{model_save_dir}train_embed_flip_fold{fold}.npy')\n",
    "        val_embed_flip_features = np.load(f'{model_save_dir}val_embed_flip_fold{fold}.npy')\n",
    "        train_embed_merge_features = np.load(f'{model_save_dir}train_embed_merge_org_fold{fold}.npy')\n",
    "        val_embed_merge_features = np.load(f'{model_save_dir}val_embed_merge_org_fold{fold}.npy')\n",
    "        train_embed_merge_flip_features = np.load(f'{model_save_dir}train_embed_merge_flip_fold{fold}.npy')\n",
    "        val_embed_merge_flip_features = np.load(f'{model_save_dir}val_embed_merge_flip_fold{fold}.npy')\n",
    "        swin_score = mean_squared_error(val_df.Pawpularity,val_predicts)**0.5\n",
    "        swin_scores.append(swin_score)\n",
    "\n",
    "        train_concat_features = np.concatenate((train_embed_merge_features, train_embed_merge_flip_features), axis=0)\n",
    "        train_loader = DataLoader(CustomDataset(train_concat_features, pd.concat([train_df['binary'], train_df['binary']]).reset_index(drop=True)), shuffle=config.train_loader.shuffle, batch_size=config.train_loader.batch_size)\n",
    "\n",
    "        # Model\n",
    "        q = nn.Sequential(\n",
    "                nn.Dropout(config.model.first_drop), \n",
    "                nn.Linear(train_concat_features.shape[1], config.model.h1),\n",
    "                eval(config.model.activation)(),\n",
    "                nn.Dropout(config.model.second_drop), \n",
    "                nn.Linear(config.model.h1, config.model.h2),\n",
    "                eval(config.model.activation)(),\n",
    "                nn.Dropout(config.model.third_drop), \n",
    "                nn.Linear(config.model.h2, 1)\n",
    "        ).to('cuda')\n",
    "\n",
    "        # Optimizer & scheduler\n",
    "        optimizer = eval(config.optimizer.name)(q.parameters(), **config.optimizer.AdamW)\n",
    "        scheduler = eval(config.scheduler.name)(optimizer, **config.scheduler.CosineAnnealingWarmRestarts)\n",
    "\n",
    "        for epoch in range(config.epoch):\n",
    "            q.train()\n",
    "            train_loss = 0\n",
    "            for feature, label in train_loader:\n",
    "            \n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "                feature, label = feature.float().to('cuda') , label.float().to('cuda')\n",
    "            \n",
    "                pred = q(feature)\n",
    "            \n",
    "                loss = criterion(pred.squeeze(dim=-1), label)\n",
    "            \n",
    "                loss.backward()\n",
    "            \n",
    "                optimizer.step()\n",
    "            \n",
    "                scheduler.step()\n",
    "            \n",
    "                train_loss += loss.item()\n",
    "            \n",
    "            q.eval()\n",
    "            with torch.no_grad():\n",
    "                org_feature = torch.FloatTensor(val_embed_merge_features).to('cuda')\n",
    "                flip_feature = torch.FloatTensor(val_embed_merge_flip_features).to('cuda')\n",
    "                org_pred = q(org_feature)\n",
    "                flip_pred = q(flip_feature)\n",
    "                org_pred = (torch.sigmoid(org_pred).ge(threshold).int().cpu().squeeze(dim=-1)).numpy()\n",
    "                flip_pred = (torch.sigmoid(flip_pred).ge(threshold).int().cpu().squeeze(dim=-1)).numpy()\n",
    "            label = val_df['binary'].to_numpy().astype(int)\n",
    "            pred_arr = (org_pred + flip_pred) == 2\n",
    "            val_cm = confusion_matrix(label, pred_arr.astype(int))\n",
    "            val_f1_score = f1_score(label,pred_arr.astype(int), average='micro')\n",
    "            \n",
    "            new_val_predicts = copy.copy(val_predicts)\n",
    "            new_val_predicts[pred_arr] = 100\n",
    "            val_rmse = mean_squared_error(val_df.Pawpularity, new_val_predicts)**0.5\n",
    "            TNR = 100 * val_cm[0][0] / (val_cm[0][0] + val_cm[0][1])\n",
    "            TPR = 100 * val_cm[1][1] / (val_cm[1][0] + val_cm[1][1])\n",
    "\n",
    "            early_stopper.check_early_stopping(loss=val_rmse)\n",
    "\n",
    "            if early_stopper.patience_counter == 0:\n",
    "                min_TNR = TNR\n",
    "                min_TPR = TPR\n",
    "                min_f1 = val_f1_score\n",
    "                val_min_score = val_rmse\n",
    "                print(f'fold {fold} epoch {epoch} train_ratio : {train_ratio:.4f} TNR : {TNR:.4f} TPR : {TPR:.4f}, F1 : {val_f1_score:.4f}, rmse : {val_rmse:.4f}, orig_rmse : {swin_score:.4f}', end = '\\r')\n",
    "            #    recorder.save_weight(epoch=epoch_index)\n",
    "\n",
    "            if early_stopper.stop == True:\n",
    "                break\n",
    "        trial.report(val_min_score, fold)\n",
    "            \n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "        val_scores.append(val_min_score)\n",
    "        print(f'fold {fold} epoch {epoch} train_ratio : {train_ratio:.4f} TNR : {min_TNR:.4f} TPR : {min_TPR:.4f}, F1 : {min_f1:.4f}, rmse : {val_min_score:.4f}, orig_rmse : {swin_score:.4f}')\n",
    "    \n",
    "    return sum(val_scores)/len(val_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2435a68-bac3-4ba3-9d2c-390d8116b9dc",
   "metadata": {},
   "source": [
    "# Study Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "27a6b887-e46e-4156-adeb-9a9a8f6bd82f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial:0\n",
      "  Params: \n",
      "    learning_rate: 9.189526625323466e-05\n",
      "    h1: 2048\n",
      "    h2: 1024\n",
      "    patience: 10\n",
      "    activation: nn.CELU\n",
      "    first_drop: 0.4171070908036911\n",
      "    second_drop: 0.8643693776266419\n",
      "    third_drop: 0.7816213051204456\n",
      "    beta1: 0.9090950616904743\n",
      "    beta2: 0.8434384048518252\n",
      "    binary_th: 85\n",
      "    weight_decay: 0.060793221076683945\n",
      "    amsgrad: False\n",
      "    T_0: 97\n",
      "    T_mult: 36\n",
      "    eta_min: 6.917351502150616e-06\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction=\"minimize\", pruner=optuna.pruners.MedianPruner())\n",
    "study.optimize(objective, n_trials=2)\n",
    "study_save_dir = f'{drive_root}/output/weights/final_weights/{model_code}_2nd_head/binary_head/'\n",
    "\n",
    "with open(f\"{study_save_dir}04-binary-head.pkl\", 'wb') as file:\n",
    "    pickle.dump(study, file)\n",
    "\n",
    "trial = study.best_trial\n",
    "print(f\"Best trial:{trial.number}\")\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0204d2-0b14-4290-93e4-e35b6fbbce3d",
   "metadata": {},
   "source": [
    "# Train & Save Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9a097a23-12cb-470a-947f-d8e1aa699602",
   "metadata": {},
   "outputs": [],
   "source": [
    "study_save_dir = f'{drive_root}/output/weights/final_weights/{model_code}_2nd_head/binary_head/'\n",
    "\n",
    "with open(f\"{study_save_dir}04-binary-head.pkl\", 'rb') as file:\n",
    "    study = pickle.load(file)\n",
    "trial = study.best_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eb50b020-3d45-48e2-878a-4e59062351af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0 epoch 10 train_ratio : 5.8313 TNR : 97.9636 TPR : 25.4237, F1 : 18.6309rmse : 18.6309, orig_rmse : 17.6382\n",
      "fold 1 epoch 10 train_ratio : 5.8313 TNR : 98.3906 TPR : 16.6667, F1 : 17.9672rmse : 17.9672, orig_rmse : 17.2808\n",
      "fold 2 epoch 10 train_ratio : 5.8313 TNR : 98.4979 TPR : 25.4237, F1 : 17.7533rmse : 17.7533, orig_rmse : 17.2190\n",
      "fold 3 epoch 10 train_ratio : 5.8313 TNR : 97.5348 TPR : 27.5862, F1 : 18.0983rmse : 18.0983, orig_rmse : 17.2257\n",
      "fold 4 epoch 29 train_ratio : 5.8313 TNR : 98.1838 TPR : 25.4545, F1 : 17.9592 rmse : 17.9592, orig_rmse : 17.1174\n",
      "fold 5 epoch 14 train_ratio : 5.8313 TNR : 98.5011 TPR : 19.2982, F1 : 18.2966rmse : 18.2966, orig_rmse : 17.3406\n",
      "fold 6 epoch 14 train_ratio : 5.8313 TNR : 98.3974 TPR : 18.1818, F1 : 18.5304rmse : 18.5304, orig_rmse : 17.6458\n",
      "fold 7 epoch 10 train_ratio : 5.8313 TNR : 98.1779 TPR : 22.4138, F1 : 18.8490rmse : 18.8490, orig_rmse : 17.8239\n",
      "fold 8 epoch 10 train_ratio : 5.8313 TNR : 97.8541 TPR : 23.7288, F1 : 19.0874rmse : 19.0874, orig_rmse : 18.0491\n",
      "fold 9 epoch 10 train_ratio : 5.8313 TNR : 98.3923 TPR : 32.7586, F1 : 17.5240rmse : 17.5240, orig_rmse : 16.8705\n"
     ]
    }
   ],
   "source": [
    "swin_scores=[]\n",
    "val_scores=[]\n",
    "config.optimizer.AdamW.lr = trial.params['learning_rate']\n",
    "config.model.h1 = trial.params['h1']\n",
    "config.model.h2 = trial.params['h2']\n",
    "config.patience =  trial.params['patience']  \n",
    "config.model.activation = trial.params['activation']\n",
    "config.model.first_drop = trial.params['first_drop']\n",
    "config.model.second_drop = trial.params['second_drop'] \n",
    "config.model.third_drop = trial.params['third_drop'] \n",
    "\n",
    "beta1 = trial.params['beta1']  \n",
    "beta2 = trial.params['beta2'] \n",
    "threshold = 0.5\n",
    "binary_th = trial.suggest_int('binary_th', 80, 100)\n",
    "config.optimizer.AdamW.betas = (beta1, beta2)\n",
    "config.optimizer.AdamW.weight_decay = trial.params['weight_decay']  \n",
    "config.optimizer.AdamW.amsgrad = trial.params['amsgrad'] \n",
    "config.scheduler.CosineAnnealingWarmRestarts.T_0 = trial.params['T_0']\n",
    "config.scheduler.CosineAnnealingWarmRestarts.T_mult = trial.params['T_mult'] \n",
    "config.scheduler.CosineAnnealingWarmRestarts.eta_min = trial.params['eta_min']\n",
    "model_save_dir = f'{drive_root}/output/weights/final_weights/{model_code}/'\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(df_train[\"Id\"], df_train[\"Pawpularity\"])):\n",
    "    early_stopper = EarlyStopper(patience = config.patience, mode='max')\n",
    "    val_min_score = 1e+100\n",
    "    binary_dir = f'{drive_root}/output/weights/final_weights/{model_code}_2nd_head/binary_head/Binary_fold{fold}.ckpt'\n",
    "    train_df = df_train.loc[train_idx].reset_index(drop=True)\n",
    "    val_df = df_train.loc[val_idx].reset_index(drop=True)\n",
    "    \n",
    "    train_ratio = 100 * sum(df_train['binary'])/len(df_train['binary'])\n",
    "    val_ratio = 100 * sum(val_df['binary'])/len(val_df['binary'])\n",
    "    \n",
    "    # Load embed features\n",
    "    val_predicts = np.load(f'{model_save_dir}val_predicts_fold{fold}.npy')\n",
    "    train_embed_features = np.load(f'{model_save_dir}train_embed_org_fold{fold}.npy')\n",
    "    val_embed_features = np.load(f'{model_save_dir}val_embed_org_fold{fold}.npy')\n",
    "    train_embed_flip_features = np.load(f'{model_save_dir}train_embed_flip_fold{fold}.npy')\n",
    "    val_embed_flip_features = np.load(f'{model_save_dir}val_embed_flip_fold{fold}.npy')\n",
    "    train_embed_merge_features = np.load(f'{model_save_dir}train_embed_merge_org_fold{fold}.npy')\n",
    "    val_embed_merge_features = np.load(f'{model_save_dir}val_embed_merge_org_fold{fold}.npy')\n",
    "    train_embed_merge_flip_features = np.load(f'{model_save_dir}train_embed_merge_flip_fold{fold}.npy')\n",
    "    val_embed_merge_flip_features = np.load(f'{model_save_dir}val_embed_merge_flip_fold{fold}.npy')\n",
    "    train_concat_features = np.concatenate((train_embed_merge_features, train_embed_merge_flip_features), axis=0)\n",
    "\n",
    "\n",
    "    swin_score = mean_squared_error(val_df.Pawpularity,val_predicts)**0.5\n",
    "    swin_scores.append(swin_score)\n",
    "    train_loader = DataLoader(CustomDataset(train_concat_features, pd.concat([train_df['binary'], train_df['binary']]).reset_index(drop=True)), shuffle=config.train_loader.shuffle, batch_size=config.train_loader.batch_size)\n",
    "    \n",
    "    # Model\n",
    "    q = nn.Sequential(\n",
    "            nn.Dropout(config.model.first_drop), \n",
    "            nn.Linear(train_concat_features.shape[1], config.model.h1),\n",
    "            eval(config.model.activation)(),\n",
    "            nn.Dropout(config.model.second_drop), \n",
    "            nn.Linear(config.model.h1, config.model.h2),\n",
    "            eval(config.model.activation)(),\n",
    "            nn.Dropout(config.model.third_drop), \n",
    "            nn.Linear(config.model.h2, 1)\n",
    "    ).to('cuda')\n",
    "        \n",
    "    # Optimizer & scheduler\n",
    "    optimizer = eval(config.optimizer.name)(q.parameters(), **config.optimizer.AdamW)\n",
    "    scheduler = eval(config.scheduler.name)(optimizer, **config.scheduler.CosineAnnealingWarmRestarts)\n",
    "\n",
    "    \n",
    "    for epoch in range(config.quantile.epoch):\n",
    "        q.train()\n",
    "        train_loss = 0\n",
    "        for feature, label in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            feature, label = feature.float().to('cuda') , label.float().to('cuda')\n",
    "            pred = q(feature)\n",
    "            loss = criterion(pred.squeeze(dim=-1), label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            train_loss += loss.item()\n",
    "        q.eval()\n",
    "        with torch.no_grad():\n",
    "            org_feature = torch.FloatTensor(val_embed_merge_features).to('cuda')\n",
    "            flip_feature = torch.FloatTensor(val_embed_merge_flip_features).to('cuda')\n",
    "            org_pred = q(org_feature)\n",
    "            flip_pred = q(flip_feature)\n",
    "            org_pred = (torch.sigmoid(org_pred).ge(threshold).int().cpu().squeeze(dim=-1)).numpy()\n",
    "            flip_pred = (torch.sigmoid(flip_pred).ge(threshold).int().cpu().squeeze(dim=-1)).numpy()\n",
    "        label = val_df['binary'].to_numpy().astype(int)\n",
    "        pred_arr = (org_pred + flip_pred) == 2\n",
    "        val_cm = confusion_matrix(label, pred_arr.astype(int))\n",
    "        val_f1_score = f1_score(label,pred_arr.astype(int), average='micro')\n",
    "        new_val_predicts = copy.copy(val_predicts)\n",
    "        new_val_predicts[pred_arr] = 100\n",
    "        val_rmse = mean_squared_error(val_df.Pawpularity, new_val_predicts)**0.5\n",
    "        TNR = 100 * val_cm[0][0] / (val_cm[0][0] + val_cm[0][1])\n",
    "        TPR = 100 * val_cm[1][1] / (val_cm[1][0] + val_cm[1][1])\n",
    "\n",
    "        early_stopper.check_early_stopping(loss=val_rmse)\n",
    "        if early_stopper.patience_counter == 0:\n",
    "            pickle.dump(q, open(binary_dir, \"wb\"))\n",
    "            min_TNR = TNR\n",
    "            min_TPR = TPR\n",
    "            min_f1 = val_f1_score\n",
    "            val_min_score = val_rmse\n",
    "            print(f'fold {fold} epoch {epoch} train_ratio : {train_ratio:.4f} TNR : {TNR:.4f} TPR : {TPR:.4f}, F1 : {val_f1_score:.4f}, rmse : {val_rmse:.4f}, orig_rmse : {swin_score:.4f}', end = '\\r')\n",
    "\n",
    "        if early_stopper.stop == True:\n",
    "            break\n",
    "    #val_scores.append(val_min_score)\n",
    "    print(f'fold {fold} epoch {epoch} train_ratio : {train_ratio:.4f} TNR : {min_TNR:.4f} TPR : {min_TPR:.4f}, F1 : {val_min_score:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6662e871-935e-4218-9ec9-3d75cf9e226b",
   "metadata": {},
   "source": [
    "# Make prediction file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1fa62515-4e10-412c-b09d-9036b1c8fcfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0 score : 18.630930755082197\n",
      "fold 1 score : 17.967236008159823\n",
      "fold 2 score : 17.75328425400149\n",
      "fold 3 score : 18.09830837005364\n",
      "fold 4 score : 17.959232481437045\n",
      "fold 5 score : 18.29655306603154\n",
      "fold 6 score : 18.530408905517888\n",
      "fold 7 score : 18.849023969863396\n",
      "fold 8 score : 19.087383109388046\n",
      "fold 9 score : 17.52397890712688\n",
      "cv score : 18.2696\n"
     ]
    }
   ],
   "source": [
    "\n",
    "skf = StratifiedKFold(\n",
    "    n_splits=config.n_splits, shuffle=True, random_state=config.seed\n",
    ")\n",
    "orig_scores = []\n",
    "scores = []\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(df_train[\"Id\"], df_train[\"Pawpularity\"])):\n",
    "    model_save_dir = f'{drive_root}/output/weights/final_weights/{model_code}/'\n",
    "    head_dir = f'{drive_root}/output/weights/final_weights/{model_code}_2nd_head/binary_head/Binary_fold{fold}.ckpt'\n",
    "    result_save_dir = f'{drive_root}/output/weights/final_weights/{model_code}_2nd_head/binary_head/'\n",
    "\n",
    "    train_df = df_train.loc[train_idx].reset_index(drop=True)\n",
    "    val_df = df_train.loc[val_idx].reset_index(drop=True)\n",
    "\n",
    "    # Load embed features\n",
    "    val_predicts = np.load(f'{model_save_dir}val_predicts_fold{fold}.npy')\n",
    "    val_embed_features = np.load(f'{model_save_dir}val_embed_org_fold{fold}.npy')\n",
    "    val_embed_flip_features = np.load(f'{model_save_dir}val_embed_flip_fold{fold}.npy')\n",
    "    val_embed_merge_features = np.load(f'{model_save_dir}val_embed_merge_org_fold{fold}.npy')\n",
    "    val_embed_merge_flip_features = np.load(f'{model_save_dir}val_embed_merge_flip_fold{fold}.npy')\n",
    "    \n",
    "    org_feature = torch.FloatTensor(val_embed_merge_features).to('cuda')\n",
    "    flip_feature = torch.FloatTensor(val_embed_merge_flip_features).to('cuda')\n",
    "    q = pickle.load(open(head_dir, \"rb\"))\n",
    "    q.eval()\n",
    "    with torch.no_grad():\n",
    "        org_pred = q(org_feature)\n",
    "        flip_pred = q(flip_feature)\n",
    "        org_pred = (torch.sigmoid(org_pred).ge(threshold).int().cpu().squeeze(dim=-1)).numpy()\n",
    "        flip_pred = (torch.sigmoid(flip_pred).ge(threshold).int().cpu().squeeze(dim=-1)).numpy()\n",
    "    pred_arr = (org_pred + flip_pred) == 2\n",
    "    new_val_predicts = copy.copy(val_predicts)\n",
    "    new_val_predicts[pred_arr] = 100\n",
    "    val_rmse = mean_squared_error(val_df.Pawpularity, new_val_predicts)**0.5\n",
    "    print(f'fold {fold} score : {val_rmse}')\n",
    "    np.save(f'{result_save_dir}val_binary_predicts{fold}.npy', pred_arr)\n",
    "    scores.append(val_rmse)\n",
    "cv_score = sum(scores)/len(scores)\n",
    "print(f'cv score : {cv_score:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4827.079628,
   "end_time": "2021-11-28T14:45:21.922531",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-11-28T13:24:54.842903",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
